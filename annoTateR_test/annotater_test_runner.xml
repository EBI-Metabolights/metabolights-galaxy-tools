<tool id="annotater_nmr_test" name="annoTateR NMR pipeline" version="0.1.0+galaxy0">
    <description>
        Workflows for automatic processing and annotation of 1D 1H-NMR spectra from MetaboLights repository.    
        </description>
    <requirements>
        <requirement type="package" version="2.4.4">r-devtools</requirement>
        <requirement type="package" version="1.8.7">r-plyr</requirement>
    </requirements>
    <stdio>
        <exit_code range="1" level="fatal" />
    </stdio>
    <command>
        <![CDATA[
        LC_ALL=C Rscript $__tool_directory__/annotater_test_runner.r
        params_file '$study_params.config_yaml'
        matrix '$study_params.spectral_matrix'
        gissmo_ref '$study_params.gissmo_ref'
        ;
        return=\$?;
        cat 'log.txt';
        sh -c "exit \$return" 
        ]]>
    </command>
    <inputs>
        <section name="study_params" title="Study" help="Params pertaining to which study to use" expanded="True">
            <!--STILL NEED TO MAKE THE CHANGE IN PIPELINE TO REFER TO THE SPECIFIC GISSMO SLICE!!!!!! -->
            <param name="gissmo_ref" type="library_data" label="Gissmo Slice" data_ref="e27d6b080c4f7867" help="Select a gissmo slice from the Gissmo ref files data library." />
            <!--STILL NEED TO MAKE THE CHANGE IN PIPELINE TO REFER TO THE SPECIFIC GISSMO SLICE!!!!!! -->
            <param name="spectral_matrix" type="library_data" label="Input Spectral Matrix" data_ref="28747fb6b1b59dd4" help="Select an input spectral matrix from the MTBLS Spectral Matrices data library." />
            <param name="config_yaml" type="file" label="Select YAML File" format="file" help="Select a YAML file containing the required configuration." />

        </section>

    </inputs>
    <outputs>
        <data name="lib.info" format=".RDS" label="${spectral_matrix.name.rsplit('.', 1)[0]}_lib.info.RDS" from_work_dir="lib.info.RDS"/>
        <data name="fse_features.pdf" format=".pdf" label="${spectral_matrix.name.rsplit('.', 1)[0]}_fse_features.pdf" from_work_dir="fse_features.pdf"/> 
        <data name="corrpeak_distribution.pdf" format=".pdf" label="${spectral_matrix.name.rsplit('.', 1)[0]}_corrpeak_distribution.pdf" from_work_dir="corrpeak_distribution.pdf"/>
        <data name="fse.result" format=".RDS" label="${spectral_matrix.name.rsplit('.', 1)[0]}_fse_result.RDS" from_work_dir="fse.result.RDS"/>
        <data name="filtered.features" format=".pdf" label="${spectral_matrix.name.rsplit('.', 1)[0]}_filtered_features.pdf" from_work_dir="filtered.features.pdf"/>
        <data name="feature.ranges" format=".pdf" label="${spectral_matrix.name.rsplit('.', 1)[0]}_feature_ranges.pdf" from_work_dir="feature.ranges.pdf"/>
        <data name="optics.opt.epsvalue" format=".pdf" label="${spectral_matrix.name.rsplit('.', 1)[0]}_optics_opt_epsvalue.pdf" from_work_dir="optics.opt.epsvalue"/>
        <data name="optics.cluster.sizeDistribution" format=".pdf" label="${spectral_matrix.name.rsplit('.', 1)[0]}_optics_cluster_size_distribution.pdf" from_work_dir="optics.cluster.sizeDistribution.pdf"/>
        <data name="feature_clusters" format=".pdf" label="${spectral_matrix.name.rsplit('.', 1)[0]}_feature_clusters.pdf" from_work_dir="feature_clusters.pdf"/>
        <data name="checked.clusters" format=".pdf" label="${spectral_matrix.name.rsplit('.', 1)[0]}_checked_clusters.pdf" from_work_dir="checked.cluster.pdf"/>
        <data name="lib.data.processed" format=".RDS" label="${spectral_matrix.name.rsplit('.', 1)[0]}_lib_data_processed.RDS" from_work_dir="lib.data.processed.RDS"/>
        <data name="match_scores_sample_x_compound.pdf" label="${spectral_matrix.name.rsplit('.', 1)[0]}_match_scores_sample_x_compound.pdf" from_work_dir="match_scores_sample_x_compound.pdf"/>
        <data name="ss.ref.sumScores" format=".RDS" label="${spectral_matrix.name.rsplit('.', 1)[0]}_ss_ref_sum_scores.RDS" from_work_dir="ss.ref.sumScores.RDS"/>
        <data name="rfs.used" format=".RDS" label="${spectral_matrix.name.rsplit('.', 1)[0]}_rfs_used.RDS" from_work_dir="rfs.used.RDS"/>
        <data name="backfit.results" format=".RDS" label="${spectral_matrix.name.rsplit('.', 1)[0]}_backfit_results.RDS" from_work_dir="backfit.results.RDS"/>

    </outputs>
    <help>
        <![CDATA[
            

        FSE: the spectral matrix is decomposed into compound features using feature
        shape extraction. First, a local STOCSY is performed at every point along the
        spectrum (within a sliding window of ~ 100 points; enough to capture multiple
        resonances within any multiplet). For each of these STOCSYs, the central peak
        in the correlation profile (correlation pocket; corrpocket) typically captures
        a resonance, as the correlation is 1 by definition at the STOCSY'd point, and
        typically falls off as you approach the boundaries of the resonance. This is
        taken, with the next highest correlation peak within the window, to form a
        rough statistical description of two resonances which have an correlation in
        intensity across samples, albeit separated by chemical shift. We term this a
        'protofeature'. Importantly, each point and its associated window will capture
        the dominant protofeature most associated with that point. This changes for
        adjacent points, and many protofeatures will be duplicated multiple times. A
        protofeature should be considered as a rough hypothesis about a statistical
        association between two resonances, which happen to be sufficiently aligned
        so that they produce a coherent signal.
        If the windows are all aligned, we can plot the % of central corrpeaks containing
        each window point. From this distribution, it is clear that nearly all
        protofeatures, including those from noise peaks and real peaks, include the
        most central window points. As such, these cannot reliably be used to identify
        noise. However, the correlation peak about noise tends to be much smaller, and
        characteristically so. As such, a noisewidth can be estimated from this
        distribution. This is the origin of the noise.percentile cutoff, which is applied
        like so: "given a noise.percentile = 0.99, consider only those protofeatures
        for which both peaks have a width > the smallest 1% of peaks". Reducing this
        number therefore gives a more selective cut. Protofeatures are also filtered so
        that the
        
        STORM: Joram Posma's STORM has been adapted and optimized to accept these
        protofeatures in the following ways:
        
        first, since many of the protofeatures are noise, we provide failure modes
        and reporting for the following cases:
        "empty subset", # empty subset (no spectrum contains signature)
        "subset degenerated", # 1-3 spectra in the subset (not enough spectra to
        get a reliable correlation)
        "reference degenerated", # signature degenerates to include < 3 points (not
        meaningful to correlate shapes)
        "did not converge" # subset continues to change after 24 iterations
        
        additionally, the correlation r and p-value cutoff q are both used during
        both the subset selection and reference update steps.
        
        we also remove any regions of the reference for which there are fewer than
        minpeak values after r and p value thresholding. This helps avoid noise.
        
        finally, STORM uses the max covariance value of the ref shape in each iteration
        as the driver. This can cause the algorithm to get lured away from small
        features, to nearby features with larger covariance. To mitigate this effect,
        we require that each driver update step remain on the same covariance signal
        peak throughout all iterations. Bear in mind that other nearby peaks will be
        assessed if they are part of a correlation pocket.
        
        STORM extracts meaningful features using protofeatures to define the region of
        interest and a rough sketch of the feature shape highly correlated with each
        spectral point. In the future, HCA could be used to cluster potential starting
        feature shapes correlated with each driver, or the nonoptimal subset for each
        point could be re-STORMed to detect any other feature shapes present. It is
        also perfectly reasonable to combine feature shapes from different STORM runs
        for a given dataset, as these comprise a list of somewhat independently tested
        feature shapes, and duplication is not an issue.
        
        TINA (TINA Is Not Alignment):
        
        We cannot eliminate all poor shapes, but there are a couple of useful heuristics
        which generally reduce unnecessary computation downstream. First, there are many
        feature shapes which are either quite poor in quality, or do not contain
        sufficient information to be useful for annotation. We keep features with:
        
        in defined ppm range (generally [-1, 11])
        long enough runs? (contains runs > noisewidth*3 adjacent points)
        large enough subset? (>= 5 spectra with feature, good correlation reliability)
        has at least 1 true peak > 0.3 of the range of intensities? (not just the
        side of a broad peak; not monotonic)
        Since the same features will often be extracted multiple times (either in the
        same spectral region, or other regions; i.e. same peak, but misaligned), it is
        advantageous to reduce this redundancy by clustering feature shapes. We accomplish
        this using a combination of UMAP projection and Affinity Propagation clustering.
        Before comparing feature shapes, we align them to their maximum intensity
        resonance. This is quick, and usually performs well enough.
        UMAP uses euclidean distance as a default. In practice, UMAP is able to sort
        feature shapes into tight clusters when low n_neighbors (e.g. 5) and min_dist
        (e.g. 0.05) are used. This does not capture global relationships as well, but
        we only use it to identify tight clusters.
        All pairwise correlations (PCCs) are calculated for the feature shapes. A mask
        for correlation thresholding is applied to the distance matrix (generated using
        apcluster::negDistMat(pts, r=2), squared negative euclidean distance) to ensure
        that clustered features have a high correlation as well. apcluster uses a q
        parameter to optimize the initial preferences. Higher q -> stricter clusters.
        Raising the lambda (dampening) parameter helps avoid oscillations which prevent
        convergence, although raising this too high can make updates too slow to
        converge within the number of iterations.
        See
        https://cran.r-project.org/web/packages/apcluster/vignettes/apcluster.pdf for
        a full description of affinity propagation parameters
        Ultimately, it doesn't matter much what clustering method is used, as this is
        primarily a means of combining highly similar features to reduce the computational
        burden of pairwise comparisons to reference spectra.
        Matching:
        Matching is accomplished by cross-correlating all pairwise feature - reference
        spectrum pairs. Since there are feature-level comparisons to make (i.e. a single
        feature across all matches), iteration over features is serial. Iteration over
        references for each feature is done in parallel. This comparison is not optimal,
        but works as a proof of concept and can be scaled up. Increasing numbers of
        features results in a linear increase in computational time, but also more memory
        usage for each core. Generally, it is advisable to leave 1 or 2 cores on one's
        machine free for system operations and the main R instance. For each comparison
        that is, for each feature - reference pair being tested, the top max.hits
        (e.g., 5) convolution hits are assessed for pearson correlation coefficient
        (r.thresh) and pvalue (p.thresh). For hits passing both thresholds, the feature
        is fit to the reference using least squares. RMSE is reported.
        
        Next, it's best to avoid penalizing a feature fit just because it contains false
        positive points. Once a feature has been fit to all reference spectra (at 0-5
        more places), resonances which were never fit are identified using peak poorness:
        
        Line up all fits for that feature across the database.
        For each fit, take the positive residuals, divide by feature intensity. If
        values are close to 1, the feature was completely absent in the ref
        signature at those point. This is a measure of ~ peak poorness for this database
        Take the mean of those values across all fits for each point in the feature.
        Square peak-poorness to squash them down unless very high:
        peak.quality = 1-(peak.poorness^2)
        For each fit's non-missing values:
        rmse.weighted = sum(residuals * peak.quality)/n.points
        This metric appears to give more intuitive results. Missing reference resonances
        compared to the feature is preferable to the opposite, which will be measured
        during backfitting.
        Matches are written to file.
        Note: matching in parallel is much lighter (~50%) if run in a new R instance
        outside of RStudio due to memory leaks/overhead.
        
        MTJ 2023
]]>
    </help>
</tool>